# 3. 손실 줄이기

참고문헌 : 
1. 구글 머신러닝 단기 집중과정

[Google의 머신러닝 단기 집중과정](https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko)

## 반복 방식

손실을 줄여 최적의 모델(최적의 w와 b)을 찾기 위한 방식이다.

처음에는 임의의 지점에서 시작해서 시스템이 손실 값을 알려줄 때까지 기다리고 그 후에 다른 값을 추정해서 손실 값을 확인한다. 이렇게 하면 점점 최적의 모델과 가까워지게 된다.

![3-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-1.PNG)

## 경사하강법

![2-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/2-3.PNG)

선형 모델에서는 위의 식과 같은 L2 손실함수를 사용한다고 하였으므로 다음과 같은 볼록 함수 모양의 손실 대 가중치 도표가 산출된다.

![3-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-2.PNG)

볼록 문제에서는 기울기가 정확하게 0인 지점인 최소값이 하나만 존재한다. 즉, 이 최소값에서 손실이 가장 작은 최적의 모델을 찾게 되는 것이다.

하지만 전체 데이터 세트에 대해 가능한 모든 w 값에 대해 손실을 계산하는 것은 아주 비효율적인 방법이다.

따라서, 우리는 **경사하강법**이라는 방법을 사용한다.

경사하강법의 첫 번째 단계는 w1에 대한 시작 값(시작점)을 선택하는 것이다. 시작점은 별로 중요하지 않기에 w1을 0으로 설정하거나 임의의 값을 선택한다.

다음 그림에서는 0보다 조금 큰 시작점을 지정하였다.

![3-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-3.PNG)

그 다음 경사하강법 알고리즘은 시작점에서 손실 곡선의 기울기를 계산한다. 간단히 설명하자면 기울기는 편미분의 벡터로서 어느 방향이 더 정확한지 혹은 더 부정확한지 알려준다.

다음 그림은 기울기 값을 계산한 것이다.

![3-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-4.PNG)

이 때, 경사하강법 알고리즘은 가능한 한 빨리 손실을 줄이기 위해 기울기의 반대 방향으로 이동한다.

그 다음, 손실 함수 곡선의 다음 지점을 결정하기 위해 경사하강법 알고리즘은 다음과 같이 기울기의 크기의 일부를 시작점에 더한다.

![3-5](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-5.PNG)

이러한 방식을 반복해 최소값에 점점 접근하게 된다.

## 학습률

앞에서 살펴본 것처럼 기울기 벡터는 방향과 크기를 모두 갖는데, 경사하강법 알고리즘은 기울기에 **학습률** 또는 **보폭**이라 불리는 스칼라를 곱하여 다음 지점을 결정한다. 예를 들어, 기울기가 2.5이고 학습률이 0.01이면 경사하강법 알고리즘은 이전 지점으로부터 0.025 떨어진 지점을 다음 지점으로 결정한다.

**초매개변수**는 프로그래머가 머신러닝 알고리즘에서 조정하는 값이다. 이 값을 조정하는데 머신러닝 프로그래머는 상당한 시간을 소비한다.

즉, 학습률을 너무 작게 설정하면 학습 시간이 매우 오래 걸리고 학습률이 너무 크다면 곡선의 최저점을 이탈하게 되기 때문에 우리는 초매개변수를 잘 선택해야하는 것이다.

![3-6](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-6.PNG)

![3-7](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-7.PNG)

결론적으로 손실 함수의 기울기가 작다면 더 큰 학습률을 시도해 볼 수 있을 것이고 기울기가 크다면 반대로 학습률을 적용해 볼 수 있을 것이다. 이렇게 우리는 학습률을 적절하게 선택해주어야 한다.

![3-8](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/3-8.PNG)

## 확률적 경사하강법

**배치:** 경사하강법에서 단일 반복의 기울기를 계산하는 데 사용하는 데이터의 총 개수이다. 즉, 지금까지는 배치가 전체 데이터 셋(전체 배치 반복)이라고 생각했지만 이 데이터 셋을 쪼개서 여러개의 배치로 만들 수 있는 것이다.

**배치 크기:** 배치 하나에 포함되는 데이터의 개수를 의미한다.

실제로 배치 크기가 커지면 데이터 중복의 가능성이 그만큼 높아진다. 따라서, 이를 적절히 조정하는 것이 중요하다.

또한, 우리가 지금까지 사용해오던 **전체 배치 반복. 즉, 모든 데이터셋을 하나의 배치에서 학습시키는 것은 대량의 데이터셋에서는 계산하는데 오랜 시간이 걸리기에 비효율적이다.**

이를 해결하기 위해 **확률적 경사하강법**(SGD)은 한 반복당 하나의 데이터(배치 크기 1)만을 사용한다. 반복이 충분하다면 SGD는 효과가 있지만 노이즈가 매우 심하다고 한다. 이 때, 데이터는 무작위로 선택한다.

우리가 사용하는 절충안은 **미니 배치 확률적 경사하강법**(미니 배치SGD)이다. 이는 전체 배치 반복과 SGD 간의 절충안인데, **미니 배치는 일반적으로 무작위로 선택한 10개에서 1000개의 데이터로 구성**된다. 미니 배치 SGD는 SGD의 노이즈를 줄이면서도 전체 배치보다는 더 효율적이다.