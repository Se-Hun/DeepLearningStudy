# 13. Dropout

참고문헌 : 
1. 구글 머신러닝 단기 집중과정
2. Kim Sung Hun 교수님의 모두를 위한 딥러닝 강의

[Google의 머신러닝 단기 집중과정](https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko)

[모두를 위한 딥러닝](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)

## Overfitting 문제

![13-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/13-1.PNG)

Overfitting을 알 수 있는 기준이 무엇일까?

Trainning Set에 대해서는 정확도가 거의 100퍼센트인데 반해 Test Set에 대해서는 정확도가 낮다면 Overfitting이 일어났음을 알 수 있다.

Nerual Network에서는 **Layer가 많아질수록 Trainning Set에 대해서는 정확도가 높아지지만 Test Set에 대해서는 정확도가 높아지다가 낮아진다고 한다.**

즉, **Network를 깊게 만들수록 Overfitting될 가능성이 높아지는 것이다!**

## 해결방법

첫째, 데이터 수 늘리기

둘째, Feature 수 줄이기

셋째, Regularization

넷째, Dropout

## Dropout

Dropout은 다음 그림과 같이 예측 모델을 만들 때 몇 개의 Layer는 끊어버리는 것을 의미한다.

이처럼 Dropout을 사용하면 **어떤 단계에서 랜덤하게 몇 개의 Layer는 예측 모델을 만들 때 사용되지 않는다.**

![13-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/13-2.PNG)

어떻게 Dropout이 Overfitting을 줄여줄 수 있을까?

다음 그림을 보자.

![13-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/13-3.PNG)

Overfitting은 너무 학습 데이터에만 맞추어지는 것을 의미한다.

이는 어떻게 생각해보면 새로운 데이터에는 필요없는 너무 많은 정보를 가지고 있기 때문일 것이다.

이렇게 Dropout을 사용하면 불필요한 너무 많은 정보를 쳐낼 수 있게 된다!!

Tensorflow에서는 다음과 같이 구현한다.

![13-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/13-4.PNG)