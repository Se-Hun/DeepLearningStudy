# 10. 신경망 학습시키기

참고문헌 : 
1. 구글 머신러닝 단기 집중과정
2. Kim Sung Hun 교수님의 모두를 위한 딥러닝 강의

[Google의 머신러닝 단기 집중과정](https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko)

[모두를 위한 딥러닝](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)

## 신경망 학습의 문제점

다음과 같은 신경망의 구조를 살펴보자.

![10-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-1.PNG)

신경망의 각 hidden layer는 w와 b값을 가진다.

**모델을 학습시킨다는 것은 적절한 w와 b값을 찾아내는 것이기에 모든 hidden layer에 대해서 적절한 w와 b값을 찾아야만 한다.**

이렇게 많은 각각의 w와 b를 계산하기에는 계산하기 너무 어려웠다.

이러한 문제때문에 딥러닝은 잊혀졌었으나, Back Propagation(역전파) 알고리즘을 통해 해결되게 된다.

## Back Propagation(역전파)

아래의 그림에서 볼 수 있듯이 앞으로 진행되던 것을 뒤에서부터 layer를 거쳐가면서 학습시키는 것이 **역전파**이다.

![10-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-2.PNG)

역전파는 무엇일까? 다음의 그림을 통해 살펴보도록 하자.

![10-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-3.PNG)

그림에서처럼 우리는 f=wx+b에서 w, x, b 각각이 f에 미치는 영향을 알고 싶다.

이는 미분을 통해 알 수 있는데, w의 f에 대한 미분 값이 5라면(df/dw = 5) 위의 그림에서처럼 w가 +1증가해서 w의 값이 -1이 된다면 f는 (-1)x5+3=-2이므로 f는 5만큼 증가하게 된다.

위의 예를 통해 알 수 있듯이, **w, x, b의 f에 대한 미분 값을 구한다면 w, x, b 각각이 f에 미치는 영향을 알 수 있다.**

즉, 모든 layer들에 대해 기울기 값을 구할 수 있기에 경사하강법을 각각의 layer에 대해 적용시킬 수 있는 것이다.

결론적으로 **우리는 다음과 같은 수식처럼 오차(첫번째 학습에서 나온 잘못된 예측값 : f)를 각 가중치로 미분한 값을 기존 가중치에서 빼줌으로써 적절한 w, b 값을 찾는 학습 과정을 진행한다.**

****

![10-9](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-9.PNG)

****

이 때, 경사하강법을 적용해 오차(cost)가 작아지는 방향으로 가는 것이다.

**그렇다면 이 미분 값은 어떻게 구할 수 있을까?**

우선, 미분에 대한 개념 하나를 살펴보고 넘어가도록 하자.

****

![10-7](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-7.PNG)

****

위의 개념은 Chain Rule이라는 개념으로, 우리에게는 합성함수의 미분법이라고 알려져있는 개념이다.

이러한 Chain Rule의 개념을 이용해서 다음과 같이 출력 layer에서부터 입력 layer로. 즉, 역방향으로 미분 값을 구해나가는 것이다.

![10-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-4.PNG)

중간 layer들에 활성화함수로 있는 시그모이드 함수도 Chain Rule의 개념으로 다음과 같이 뒤에서부터 미분해 나가면 된다.

![10-5](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-5.PNG)

이렇게 역방향으로 오차를 잡아 나가는 것이 **역전파**이다.

![10-6](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/10-6.PNG)

