# 2. 단어 embedding - word2vector

참고문헌 : 
1. Word2Vec 유튜브 강의

[Word2Vec 유튜브 강의](https://www.youtube.com/watch?v=sY4YyacSsLc)


## 자연어 처리에서의 text 처리 방식

자연어 처리에서는 text를 있는 그대로 input으로 넣어줄 수가 없다.

이 때문에 text를 숫자로 바꾸는 과정을 거치게 된다.

이를 **Encoding**이라고 한다.

![2-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/Transformer/png/2-1.PNG)

## One-hot Encoding

보통 자연어 처리에서는 one-hot encoding을 이용하여 vector로 Encoding한다.

![2-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/Transformer/png/2-2.PNG)

## One-hot Encoding의 문제점

하지만 One-hot encoding으로 encoding을 진행할 경우, **Similarity**. 즉, 유사도를 표현할 수가 없다.

예를 들어서 유사한 두 개의 단어 "고맙다-사랑한다"와 "고맙다-미워한다" 중에서 Similarity를 따져볼 때에 전자가 조금 더 가까운 관계에 있음에도 이를 One-hot encoding은 표현하지 못 한다.

![2-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/Transformer/png/2-3.PNG)

위의 그림처럼 One-hot encoding으로 표현을 하게 되면 모든 벡터간의 거리가 동일하게 된다.

따라서 새로운 방법이 필요하게 된다.

## One-hot Encoding의 대안 : Embedding

![2-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/Transformer/png/2-4.PNG)

위의 그림과 같이 Embedding 방법을 이용할 경우, **Similarity를 표현할 수 있고 One-hot Encoding보다 더 저차원 벡터를 사용할 수 있게 된다.**

## 단어 Embedding의 한 가지 방법 : Word2Vec

[Word2Vec 유튜브 강의](https://www.youtube.com/watch?v=sY4YyacSsLc)

