# 9. Deep Learning과 신경망의 등장

참고문헌 : 
1. 구글 머신러닝 단기 집중과정
2. Kim Sung Hun 교수님의 모두를 위한 딥러닝 강의

[Google의 머신러닝 단기 집중과정](https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko)

[모두를 위한 딥러닝](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)

## 선형모델의 문제점

머신러닝에서는 다음과 같은 선형식을 통해 문제를 풀 수 있었다.

![9-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-1.PNG)

**하지만 다음과 같은 복잡한 데이터는 선을 그어 풀 수 없다.**

![9-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-2.PNG)

이 때, '비선형'이라는 의미는 선형 모델과 같은 식으로 선을 정확하게 그을 수 없다는 의미이다.

## XOR 문제

이러한 비선형 문제의 대표적인 예로는 다음과 같은 XOR 문제가 있다.

![9-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-3.PNG)

위의 그림과 같은 XOR문제는 선 하나를 그어서는 풀 수 없는 것이다.

## 신경망의 등장

이러한 비선형 문제를 풀 수 없다는 이유로 선형 모델에서 신경망을 쓰게 되었다.

**딥러닝은 이러한 신경망을 쓰는 학습방법**이다.

선형모델을 그래프로 나타낸다면 다음과 같다.

![9-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-4.PNG)

**어떻게 선형 모델을 변경하여 비선형 문제 해결 능력을 개선시킬 수 있을까?**

아래 그림처럼 중간값의 '히든 레이어'를 추가해보자.

![9-5](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-5.PNG)

히든 레이어에서 각 노란색 노드는 파란색 입력 노드 값의 가중 합이다. 또한, 출력은 노란색 노드의 가중 합이다.

하지만 위 모델은 여전히 선형이다. 즉, 출력은 여전히 입력의 선형 조합인 것이다.

레이어를 다음 그림과 같이 하나 더 추가해도 똑같다.

![9-6](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-6.PNG)

## 비선형성 추가하기 : 활성화 함수

모델의 비선형 능력을 개선하기 위해 우리는 활성화 함수를 사용한다.

활성화 함수를 이용하여 모델을 개선하면 다음 그림과 같다.

![9-7](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-7.PNG)

활성화 함수는 다음과 같은 두개의 함수가 일반적으로 사용된다.

****

![9-8](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-8.PNG)

![9-9](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-9.PNG)

****

입력 특성이 다음과 같이 두 개이고 각각을 x1, x2라 하고 활성화 함수는 시그모이드 함수를 사용할 때, 각 layer는 다음과 같이 동작한다.

![9-10](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-10.PNG)

그림에서 보면 알 수 있듯이, x1과 x2는 wx+b로 계산된다. 이를 가중합이라고 하고 그렇게 만들어진 가중합은 시그모이드 함수에 대입되어 결과가 0 또는 1이 된다.

이러한 방식으로 비선형성이 추가되는 것이다.

## XOR 문제에 적용

이 모델을 가지고 XOR 문제가 풀어지는지 직접 계산을 통해 알아보도록 하자.

다음과 같은 layer를 가지는 신경망 구조를 직접 계산해자.

![9-11](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-11.PNG)

결과는 다음과 같다.

![9-12](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/9-12.PNG)

이처럼 신경망을 이용하면 XOR 문제를 풀 수 있음을 알 수 있다.

w값과 b값만 정확하다면 XOR 문제를 아주 잘 풀어내고 있음을 위의 표를 통해 알 수 있다!!