# 11. relu 함수로의 변경

참고문헌 : 
1. 구글 머신러닝 단기 집중과정
2. Kim Sung Hun 교수님의 모두를 위한 딥러닝 강의

[Google의 머신러닝 단기 집중과정](https://developers.google.com/machine-learning/crash-course/ml-intro?hl=ko)

[모두를 위한 딥러닝](https://www.youtube.com/watch?v=BS6O0zOGX4E&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm)

## 시그모이드 함수의 문제점

다음의 그림은 layer가 많아질수록 이전의 layer들이 출력 layer에 영향을 끼치지 못함을 나타내는 그림이다.

![11-1](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/11-1.PNG)

이는 **Vanishing gradient**라는 아주 유명한 문제로 딥러닝 알고리즘이 잊혀졌던 이유였다.

다음 그림을 통해 자세히 살펴보도록 하자.

![11-3](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/11-3.PNG)

위의 그림의 빨간색 네모 상자는 df/dx 값을 구한 것이다. layer가 거듭될수록 소수가 계속 곱해져 입력 layer의 gradient. 즉, 미분값이 아주 작아지고 있음을 알 수 있다.

**이렇게 미분 값이 작아짐으로써 입력 층 쪽의 layer들의 w와 b값은 출력 layer가 내놓는 예측 값에 거의 영향을 끼치지 못했다.**

결국, layer가 조금만 커져도 입력층의 layer에 대한 정보는 사라졌기에 학습이 잘되지 않았다.

이러한 문제는 시그모이드 함수가 다음과 같이 1에서 점근선을 나타내고 있기 때문이다.

![11-2](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/11-2.PNG)

## 해결책 : ReLU함수

이러한 문제를 딥러닝 학자들은 다음과 같은 ReLU함수를 활성화함수로 바꿈으로써 해결했다.

![11-4](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/11-4.PNG)

따라서, 이제부터는 ReLU함수를 활성화함수로 사용하기로 한다.

![11-5](https://github.com/Se-Hun/DeepLearningStudy/blob/master/conceptStudy/png/11-5.PNG)

실제로도 다음과 같은 그래프를 통해서 알 수 있듯이, 학습이 거듭되어도 cost(손실)값이 sigmoid는 거의 떨어지지 않았고 ReLU는 바로 떨어짐을 알 수 있다.